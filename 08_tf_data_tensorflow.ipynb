{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36bc0b41",
   "metadata": {},
   "source": [
    "**17. `tf.data` API in TensorFlow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7628102f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# import plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59c2751d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (60000,), (10000, 28, 28), (10000,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(X_train,y_train),(X_test,y_test) = mnist.load_data()\n",
    "X_train.shape,y_train.shape,X_test.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76b6806",
   "metadata": {},
   "source": [
    "**Using tf.data API from tensor-slices**\n",
    "\n",
    "1. Create a Dataset from (features,labels) using `tf.data.Dataset.from_tensor_slices((features,labels))`. \n",
    "2. If we need to map some pre-processing function, we can do it using `map()`, which maps pre-processing function row by row (one element at a time, can be parallelized to multiple elements at a time) during iteration. \n",
    "3. We can use `.cache()` to cache the data which is helpful in speeding up the process, but needs space in RAM. \n",
    "4. We can use `.shuffle()` to shuffle the data.\n",
    "5. We can use `.batch()` to create data batches.\n",
    "6. We can use `.prefetch()`, this enhances parallel efficiency.\n",
    "\n",
    "For test_dataset we can do caching after batching.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "855c43e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_TensorSliceDataset element_spec=(TensorSpec(shape=(28, 28), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.uint8, name=None))>\n",
      "<_ParallelMapDataset element_spec=(TensorSpec(shape=(28, 28, 1), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.uint8, name=None))>\n",
      "<CacheDataset element_spec=(TensorSpec(shape=(28, 28, 1), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.uint8, name=None))>\n",
      "<_ShuffleDataset element_spec=(TensorSpec(shape=(28, 28, 1), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.uint8, name=None))>\n",
      "<_ParallelBatchDataset element_spec=(TensorSpec(shape=(32, 28, 28, 1), dtype=tf.float32, name=None), TensorSpec(shape=(32,), dtype=tf.uint8, name=None))>\n",
      "<_PrefetchDataset element_spec=(TensorSpec(shape=(32, 28, 28, 1), dtype=tf.float32, name=None), TensorSpec(shape=(32,), dtype=tf.uint8, name=None))>\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "RANDOM_SEED = 14\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train,y_train))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test,y_test))\n",
    "\n",
    "print(train_dataset)\n",
    "\n",
    "# function to pre-process data\n",
    "def pre_process(img,label):\n",
    "    img = tf.expand_dims(img,axis=-1)\n",
    "    return tf.cast(img,tf.float32)/255.0, label\n",
    "\n",
    "# applying transformation\n",
    "train_dataset = train_dataset.map(pre_process,num_parallel_calls = tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.map(pre_process, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "print(train_dataset)\n",
    "\n",
    "# caching\n",
    "train_dataset = train_dataset.cache()\n",
    "test_dataset = test_dataset.cache()\n",
    "\n",
    "print(train_dataset)\n",
    "\n",
    "\n",
    "# shuffling\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1000,seed = RANDOM_SEED)\n",
    "### we don't want to shuffle the test dataset\n",
    "\n",
    "print(train_dataset)\n",
    "\n",
    "# batching\n",
    "train_dataset = train_dataset.batch(batch_size=BATCH_SIZE,drop_remainder=True,num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(batch_size=BATCH_SIZE,drop_remainder=True,num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "print(train_dataset)\n",
    "\n",
    "# prefetching\n",
    "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7b005ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 28, 28) tf.Tensor([8 2 7 8 2 5 9 7 9 1 1 6 7 5 3 5 4 1 2 6 3 8 1 6 8 2 0 6 4 3 7 5], shape=(32,), dtype=uint8)\n"
     ]
    }
   ],
   "source": [
    "# visualizing elements in dataset (before batching)\n",
    "# for img,label in train_dataset:\n",
    "#     plt.imshow(img,cmap='gray')\n",
    "#     plt.title(f'Number: {label}')\n",
    "#     plt.show()\n",
    "#     break\n",
    "\n",
    "\n",
    "for X_batch,y_batch in train_dataset:\n",
    "    print(X_batch.shape,y_batch)\n",
    "    break \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "821b7427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 3, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[4.]], dtype=float32)>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.constant(np.arange(9).reshape((3,3)))\n",
    "a = tf.expand_dims(a,axis = 0)\n",
    "a = tf.expand_dims(a,axis = -1)\n",
    "print(a.shape)\n",
    "tf.keras.layers.GlobalAveragePooling2D()(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ac8cb2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a model using the functional API\n",
    "\n",
    "def create_cnn(name = 'Model'):\n",
    "    inputs = tf.keras.Input(shape=(28,28,1))\n",
    "\n",
    "    # creating simplest convolutional-pool network\n",
    "    x = tf.keras.layers.Conv2D(filters = 16, kernel_size=(3,3), strides=(1,1), \n",
    "                               activation='relu', padding = 'same',name = 'conv1')(inputs)\n",
    "    x = tf.keras.layers.MaxPool2D(pool_size=(2,2), name = 'pool1')(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(filters = 8, kernel_size=(3,3), strides=(1,1), \n",
    "                               activation='relu', padding = 'same', name = 'conv2')(x)\n",
    "    x = tf.keras.layers.MaxPool2D(pool_size=(2,2), name = 'pool2')(x)\n",
    "    \n",
    "    # global-max-pool\n",
    "    x = tf.keras.layers.GlobalMaxPooling2D()(x)\n",
    "\n",
    "    # creating fully connected-layers\n",
    "    x = tf.keras.layers.Dense(16, activation= 'relu')(x)\n",
    "    outputs = tf.keras.layers.Dense(10,activation = 'softmax')(x)\n",
    "\n",
    "    return tf.keras.models.Model(inputs = inputs,outputs = outputs,name = name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6de90f3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ pool1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ pool2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling2d_6          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling2D</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">144</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">170</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_15 (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1 (\u001b[38;5;33mConv2D\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │           \u001b[38;5;34m160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ pool1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2 (\u001b[38;5;33mConv2D\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m8\u001b[0m)      │         \u001b[38;5;34m1,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ pool2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling2d_6          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling2D\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_26 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m144\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_27 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m170\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,634</span> (6.38 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,634\u001b[0m (6.38 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,634</span> (6.38 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,634\u001b[0m (6.38 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = create_cnn()\n",
    "model.summary()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "model.compile(loss = loss, optimizer = optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f075fa10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.3901 - loss: 1.7208 - val_accuracy: 0.7377 - val_loss: 0.7728\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.7593 - loss: 0.7327 - val_accuracy: 0.8140 - val_loss: 0.5761\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.8152 - loss: 0.5886 - val_accuracy: 0.8476 - val_loss: 0.4902\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.8439 - loss: 0.5018 - val_accuracy: 0.8662 - val_loss: 0.4356\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.8615 - loss: 0.4501 - val_accuracy: 0.8698 - val_loss: 0.4204\n",
      "CPU times: user 6min 26s, sys: 20.7 s, total: 6min 47s\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = model.fit(train_dataset,\n",
    "          epochs = 5,\n",
    "          verbose = 1,\n",
    "          validation_data = test_dataset,\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d22b20b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8724 - loss: 0.4016\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.35771411657333374, 0.8863180875778198]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c5d0a0",
   "metadata": {},
   "source": [
    "**Creating custom training loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7f570327",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = create_cnn('Model1')\n",
    "# model1.summary()\n",
    "loss_function = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "dbaa31cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.3910, Accuracy: 88.29 %\n",
      "Epoch: 2, Loss: 0.3720, Accuracy: 89.46 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-27 13:59:15.801907: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Loss: 0.3575, Accuracy: 87.39 %\n",
      "Epoch: 4, Loss: 0.3453, Accuracy: 89.84 %\n",
      "Epoch: 5, Loss: 0.3357, Accuracy: 90.07 %\n",
      "CPU times: user 10min 43s, sys: 1min 39s, total: 12min 22s\n",
      "Wall time: 6min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "def train_step(X_batch, y_batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model1(X_batch, training=True)\n",
    "        loss = loss_function(y_batch, y_pred)\n",
    "    grads = tape.gradient(loss, model1.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model1.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "for epochi in range(epochs):\n",
    "    batch_loss = []\n",
    "\n",
    "    for X_batch, y_batch in train_dataset:\n",
    "        loss = train_step(X_batch, y_batch)\n",
    "        batch_loss.append(loss)\n",
    "\n",
    "    mean_batch_loss = tf.reduce_mean(batch_loss)\n",
    "\n",
    "    val_accuracy.reset_state()  # Clear for new epoch\n",
    "    for X_val, y_val in test_dataset:\n",
    "        val_accuracy.update_state(y_val, model1(X_val, training=False))\n",
    "\n",
    "    accuracy = val_accuracy.result() * 100\n",
    "    print(f'Epoch: {epochi+1}, Loss: {mean_batch_loss:.4f}, Accuracy: {accuracy:.2f} %')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "839aadc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function train_step at 0x7d0eadd0ab00> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function train_step at 0x7d0eadd0ab00>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Epoch: 1, Loss: 0.3278, Accuracy: 90.05 %\n",
      "Epoch: 2, Loss: 0.3208, Accuracy: 90.10 %\n",
      "Epoch: 3, Loss: 0.3139, Accuracy: 90.58 %\n",
      "Epoch: 4, Loss: 0.3093, Accuracy: 90.61 %\n",
      "Epoch: 5, Loss: 0.3034, Accuracy: 89.78 %\n",
      "CPU times: user 6min 41s, sys: 19.1 s, total: 7min\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "@tf.function\n",
    "def train_step(X_batch, y_batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model1(X_batch, training=True)\n",
    "        loss = loss_function(y_batch, y_pred)\n",
    "    grads = tape.gradient(loss, model1.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model1.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "for epochi in range(epochs):\n",
    "    batch_loss = []\n",
    "\n",
    "    for X_batch, y_batch in train_dataset:\n",
    "        loss = train_step(X_batch, y_batch)\n",
    "        batch_loss.append(loss)\n",
    "\n",
    "    mean_batch_loss = tf.reduce_mean(batch_loss)\n",
    "\n",
    "    val_accuracy.reset_state()  # Clear for new epoch\n",
    "    for X_val, y_val in test_dataset:\n",
    "        val_accuracy.update_state(y_val, model1(X_val, training=False))\n",
    "\n",
    "    accuracy = val_accuracy.result() * 100\n",
    "    print(f'Epoch: {epochi+1}, Loss: {mean_batch_loss:.4f}, Accuracy: {accuracy:.2f} %')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
