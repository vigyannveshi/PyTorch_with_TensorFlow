{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f7dbe68",
   "metadata": {},
   "source": [
    "### **Automatic Differentiation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d4b565",
   "metadata": {},
   "source": [
    "**10. Autograd in PyTorch**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337051a8",
   "metadata": {},
   "source": [
    "* As nested functions get complex, determination of their derivatives and programming them become difficult.\n",
    "* Neural network is some kind of a nested function and performing backprop needs computation of immense derivatives. Computation of derivatives manually is almost impossible.\n",
    "* Autograd is a core component of Pytorch that provides automatic differentiation for tensor operations. It enables gradient computation, which is essential for training machine learning models using optimization algorithm like gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f57fb9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as tr\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# import plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d818fd7",
   "metadata": {},
   "source": [
    "**10.01. Computing gradient for scalars**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29346aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tr.tensor(3.0, requires_grad=True) # requires gradient (default - False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00e0b175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3., requires_grad=True)\n",
      "tensor(9., grad_fn=<PowBackward0>)\n",
      "tensor(6., grad_fn=<MulBackward0>)\n",
      "tensor(3., grad_fn=<MulBackward0>)\n",
      "tensor(5., grad_fn=<RsubBackward1>)\n",
      "tensor(14., grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x**2\n",
    "z = 2*x\n",
    "k = 9/x\n",
    "l = 8-x\n",
    "m = 11+x\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)\n",
    "print(k)\n",
    "print(l)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98f357f",
   "metadata": {},
   "source": [
    "* when PyTorch is told that we need to compute gradient, it at backend creates a computation graph.<br>\n",
    "  x --> (pow) --> y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98ad6d4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward() # needs to be run to compute gradient\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c030a5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3., requires_grad=True)\n",
      "tensor(9., grad_fn=<PowBackward0>)\n",
      "tensor(0.4121, grad_fn=<SinBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = tr.tensor(3.0, requires_grad=True)\n",
    "y = x**2\n",
    "z = tr.sin(y)\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74677e77",
   "metadata": {},
   "source": [
    "x-->(pow)-->y-->(sin)-->z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3eadc909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-5.4668, grad_fn=<MulBackward0>)\n",
      "tensor(-0.9111, grad_fn=<CosBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def dz_dx(x):\n",
    "    return 2*x*tr.cos(x**2)\n",
    "\n",
    "def dz_dy(y):\n",
    "    return tr.cos(y)\n",
    "\n",
    "print(dz_dx(x))\n",
    "print(dz_dy(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39e42222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tensor(-5.4668)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_137011/3451746306.py:2: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /pytorch/build/aten/src/ATen/core/TensorBody.h:489.)\n",
      "  print(y.grad)\n"
     ]
    }
   ],
   "source": [
    "z.backward()\n",
    "print(y.grad)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c90db5",
   "metadata": {},
   "source": [
    "* By default, PyTorch only stores .grad for leaf tensors. To store gradients for non-leaf tensors like y, you need to explicitly call .retain_grad() on them before backward().\n",
    "* grad can be implicitly created only for scalar outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "62ee47e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3., requires_grad=True)\n",
      "tensor(9., grad_fn=<PowBackward0>)\n",
      "tensor(0.4121, grad_fn=<SinBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = tr.tensor(3.0, requires_grad=True)\n",
    "y = x**2\n",
    "y.retain_grad() # retains gradient\n",
    "z = tr.sin(y)\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f23d30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.9111)\n",
      "tensor(-5.4668)\n"
     ]
    }
   ],
   "source": [
    "z.backward()\n",
    "\n",
    "print(y.grad)  # should now print the gradient of z w.r.t y\n",
    "print(x.grad)  # should print the gradient of z w.r.t x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea38389",
   "metadata": {},
   "source": [
    "**10.02. Computing gradients for vectors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2642210e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([0.0000, 0.1579, 0.3158, 0.4737, 0.6316, 0.7895, 0.9474, 1.1053, 1.2632,\n",
      "        1.4211, 1.5789, 1.7368, 1.8947, 2.0526, 2.2105, 2.3684, 2.5263, 2.6842,\n",
      "        2.8421, 3.0000], requires_grad=True)\n",
      "y: tensor([0.0000, 0.0249, 0.0997, 0.2244, 0.3989, 0.6233, 0.8975, 1.2216, 1.5956,\n",
      "        2.0194, 2.4931, 3.0166, 3.5900, 4.2133, 4.8864, 5.6094, 6.3823, 7.2050,\n",
      "        8.0776, 9.0000], grad_fn=<PowBackward0>)\n",
      "z: tensor([ 0.0000,  0.0249,  0.0996,  0.2225,  0.3884,  0.5837,  0.7818,  0.9397,\n",
      "         0.9997,  0.9011,  0.6040,  0.1246, -0.4336, -0.8780, -0.9849, -0.6239,\n",
      "         0.0989,  0.7967,  0.9751,  0.4121], grad_fn=<SinBackward0>)\n",
      "y.grad: tensor([ 1.0000,  0.9997,  0.9950,  0.9749,  0.9215,  0.8120,  0.6236,  0.3421,\n",
      "        -0.0248, -0.4337, -0.7970, -0.9922, -0.9011, -0.4786,  0.1732,  0.7815,\n",
      "         0.9951,  0.6044, -0.2217, -0.9111])\n",
      "x.grad: tensor([ 0.0000,  0.3157,  0.6284,  0.9236,  1.1640,  1.2821,  1.1815,  0.7563,\n",
      "        -0.0626, -1.2326, -2.5168, -3.4466, -3.4148, -1.9649,  0.7655,  3.7017,\n",
      "         5.0278,  3.2446, -1.2603, -5.4668])\n"
     ]
    }
   ],
   "source": [
    "import torch as tr\n",
    "import numpy as np\n",
    "\n",
    "x = tr.tensor(np.linspace(0, 3.0, 20), dtype=tr.float32, requires_grad=True)\n",
    "y = x**2\n",
    "y.retain_grad()\n",
    "z = tr.sin(y)\n",
    "\n",
    "print(\"x:\", x)\n",
    "print(\"y:\", y)\n",
    "print(\"z:\", z)\n",
    "\n",
    "# Backpropagate using a scalar-valued function of z\n",
    "z.sum().backward() # some kind of multi-variable differentiation w.r.t each vector column (xi)\n",
    "\n",
    "print(\"y.grad:\", y.grad)  # ∂(sum sin(y)) / ∂y = cos(y)\n",
    "print(\"x.grad:\", x.grad)  # ∂(sum sin(x^2)) / ∂x = cos(x^2) * 2x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b75c86",
   "metadata": {},
   "source": [
    "**10.03 Clearing gradients**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c519164",
   "metadata": {},
   "source": [
    "* If `backward()` function is called multiple times, the gradients get accumulated (Added), not cleared on its own. \n",
    "* Hence when multiple passes are to be run on data, we clear the gradient.\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dec8af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tr.tensor(2.0, requires_grad=True)\n",
    "y = x**2 # forward pass\n",
    "\n",
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8c66cb3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3433540d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x**2\n",
    "\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c02e353e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_() # inplace operation which assigns zero to gradient of x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c40fc1ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x**2\n",
    "\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cd1bbc",
   "metadata": {},
   "source": [
    "**10.04 Disable gradient tracking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcbfd95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.) tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "# 1. setting requires_grad_(False)\n",
    "x = tr.tensor(2.0)\n",
    "x.requires_grad_(False) # fun_ --> inplace changes\n",
    "y = x**2 # forward pass\n",
    "print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5570cb8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "tensor(2., requires_grad=True)\n",
      "tensor(4., grad_fn=<PowBackward0>)\n",
      "tensor(2.)\n",
      "tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "# 2. using detach() --> creates completely new tensor\n",
    "x = tr.tensor(2.0,requires_grad=True)\n",
    "y = x**2\n",
    "z = x.detach() \n",
    "y1 = z**2\n",
    "print(id(x)==id(z))\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)\n",
    "print(y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc39416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2., requires_grad=True) tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "# 3. using no_grad() function\n",
    "x = tr.tensor(2.0,requires_grad=True)\n",
    "\n",
    "with tr.no_grad():\n",
    "    y = x**2\n",
    "\n",
    "print(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cd115e",
   "metadata": {},
   "source": [
    "**11. GradientTape in Tensorflow**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b886f42a",
   "metadata": {},
   "source": [
    "* `tf.GradientTape` tracks operations and computes gradients via reverse-mode autodiff\n",
    "* You must wrap all computations inside the `with tf.GradientTape(...)` block.\n",
    "* To compute gradients with respect to a tensor, that tensor must be a tf.Variable (or at least explicitly marked as trainable).\n",
    "* You cannot compute gradients w.r.t. a tf.constant, because constants are not considered trainable by default. ( there are still walkthrough solutions to this)\n",
    "* In TensorFlow, a tensor is considered trainable if it is a tf.Variable with the trainable=True flag (which is the default). `x = tf.Variable(3.0, trainable=True)`\n",
    "* `persistent` is set to True if:\n",
    "  * You need multiple gradients from the same graph.\n",
    "  * You're debugging or inspecting intermediate layers.\n",
    "  * You're implementing custom training loops involving multiple partial derivatives.\n",
    "* if gradient is called only once, it may not be needed.\n",
    "* If you do use persistent=True, you should explicitly delete the tape when done\n",
    "* This helps free memory because persistent tapes retain more information than normal tapes.\n",
    "* TensorFlow allows computation of gradient of vectors without `aggregation` like sum, mean etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fb7f2e",
   "metadata": {},
   "source": [
    "**11.01 Computing gradient for scalars**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121c9bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(-0.91113025, shape=(), dtype=float32)\n",
      "tf.Tensor(-5.4667816, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 1. for tf.Variable()\n",
    "x = tf.Variable(3.0, trainable = True) # defualt, trainable is True\n",
    "\n",
    "\n",
    "# start recording gradients\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    # tape.watch(x) # if trainable is False, we need to watch the Variable\n",
    "    y = x**2\n",
    "    z = tf.sin(y)\n",
    "\n",
    "# computing gradients\n",
    "dz_dy = tape.gradient(z,y)\n",
    "dz_dx = tape.gradient(z,x)\n",
    "\n",
    "print(dz_dy)\n",
    "print(dz_dx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4be3ee75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(-0.91113025, shape=(), dtype=float32)\n",
      "tf.Tensor(-5.4667816, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 2. for tf.constant()\n",
    "x = tf.constant(3.0) \n",
    "\n",
    "# start recording gradients\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    tape.watch(x)  # manually tell tape to watch this constant\n",
    "    y = x**2\n",
    "    z = tf.sin(y)\n",
    "\n",
    "# if not watched for constant it will print 'None'\n",
    "\n",
    "# computing gradients\n",
    "dz_dy = tape.gradient(z,y)\n",
    "dz_dx = tape.gradient(z,x)\n",
    "\n",
    "print(dz_dy)\n",
    "print(dz_dx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a985f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "del tape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6f798d",
   "metadata": {},
   "source": [
    "**11.02. Computing gradients for vectors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef346a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[ 1.          0.99383351  0.90284967  0.54030231 -0.20550672 -0.93454613\n",
      " -0.65364362  0.6683999   0.67640492 -0.91113026], shape=(10,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[ 0.          0.66255567  1.20379956  1.08060461 -0.54801792 -3.11515378\n",
      " -2.61457448  3.11919955  3.60749292 -5.46678157], shape=(10,), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(tf.linspace(0,3,10), trainable = True) # defualt, trainable is True\n",
    "\n",
    "\n",
    "# start recording gradients\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    # tape.watch(x) # if trainable is False, we need to watch the Variable\n",
    "    y = x**2\n",
    "    z = tf.sin(y)\n",
    "\n",
    "# computing gradients\n",
    "dz_dy = tape.gradient(z,y)\n",
    "dz_dx = tape.gradient(z,x)\n",
    "\n",
    "print(dz_dy)\n",
    "print(dz_dx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2530ec93",
   "metadata": {},
   "source": [
    "**Notes:**\n",
    "* TensorFlow computes ∂zᵢ/∂xⱼ for each zᵢ with respect to xⱼ, and sums it up internally unless you control the upstream gradient.\n",
    "* PyTorch requires scalar output by default and computes ∂(scalar)/∂x. This is a design choice rooted in backpropagation theory, where you typically compute gradients of a scalar-valued loss function with respect to parameters. But PyTorch lets you override it by: <br>\n",
    "`z.backward(gradient=some_vector) # Explicit vector-Jacobian product`\n",
    "* PyTorch tracks gradients automatically if you opt in per tensor with `requires_grad=True`.\n",
    "* TensorFlow only tracks gradients when you explicitly record them using `tf.GradientTape()`.\n",
    "*  In TensorFlow: Gradients are freshly computed each time\n",
    "   *  Gradients do not accumulate.\n",
    "   *  When you call tape.gradient(...), you get a new set of gradients.\n",
    "   *  There is no .grad attribute on variables.\n",
    "   *  You apply gradients via an optimizer, not by modifying Variables directly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
